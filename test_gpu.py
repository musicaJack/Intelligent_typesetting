#!/usr/bin/env python3
"""
GPUæ£€æµ‹å’Œæµ‹è¯•è„šæœ¬

ç”¨äºæ£€æµ‹å’Œæµ‹è¯•GPUè®¾å¤‡ï¼Œç‰¹åˆ«æ˜¯4090Dæ˜¾å¡çš„ä½¿ç”¨æƒ…å†µã€‚
"""

import torch
import sys
from pathlib import Path

def check_gpu():
    """æ£€æŸ¥GPUè®¾å¤‡"""
    print("ğŸ” GPUè®¾å¤‡æ£€æµ‹")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡")
        print("è¯·ç¡®ä¿å·²å®‰è£…CUDAå’ŒPyTorch GPUç‰ˆæœ¬")
        return False
    
    # è·å–GPUæ•°é‡
    gpu_count = torch.cuda.device_count()
    print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUè®¾å¤‡")
    
    # æ˜¾ç¤ºæ¯ä¸ªGPUçš„è¯¦ç»†ä¿¡æ¯
    for i in range(gpu_count):
        print(f"\nğŸ“Š GPU {i} è¯¦ç»†ä¿¡æ¯:")
        print(f"   åç§°: {torch.cuda.get_device_name(i)}")
        
        # æ˜¾å­˜ä¿¡æ¯
        props = torch.cuda.get_device_properties(i)
        total_memory = props.total_memory / (1024**3)
        print(f"   æ˜¾å­˜æ€»é‡: {total_memory:.1f} GB")
        
        # è®¡ç®—èƒ½åŠ›
        compute_capability = f"{props.major}.{props.minor}"
        print(f"   è®¡ç®—èƒ½åŠ›: {compute_capability}")
        
        # å¤šå¤„ç†å™¨æ•°é‡
        print(f"   å¤šå¤„ç†å™¨: {props.multi_processor_count}")
        
        # å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        torch.cuda.set_device(i)
        allocated = torch.cuda.memory_allocated(i) / (1024**3)
        reserved = torch.cuda.memory_reserved(i) / (1024**3)
        print(f"   å½“å‰ä½¿ç”¨: {allocated:.2f} GB (å·²åˆ†é…) / {reserved:.2f} GB (å·²ä¿ç•™)")
    
    return True

def test_gpu_performance():
    """æµ‹è¯•GPUæ€§èƒ½"""
    print("\nğŸš€ GPUæ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âŒ æ— æ³•è¿›è¡ŒGPUæ€§èƒ½æµ‹è¯•")
        return
    
    device = torch.device("cuda:0")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æµ‹è¯•å¼ é‡è¿ç®—
    print("\nğŸ“ˆ å¼ é‡è¿ç®—æµ‹è¯•:")
    
    # åˆ›å»ºæµ‹è¯•å¼ é‡
    sizes = [1000, 2000, 4000]
    for size in sizes:
        print(f"   æµ‹è¯• {size}x{size} çŸ©é˜µä¹˜æ³•...")
        
        # åˆ›å»ºéšæœºçŸ©é˜µ
        a = torch.randn(size, size, device=device)
        b = torch.randn(size, size, device=device)
        
        # é¢„çƒ­
        torch.matmul(a, b)
        torch.cuda.synchronize()
        
        # è®¡æ—¶
        import time
        start_time = time.time()
        
        for _ in range(10):
            result = torch.matmul(a, b)
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 10
        print(f"   å¹³å‡æ—¶é—´: {avg_time*1000:.2f} ms")
    
    # æ˜¾å­˜æµ‹è¯•
    print("\nğŸ’¾ æ˜¾å­˜æµ‹è¯•:")
    torch.cuda.empty_cache()
    
    # æµ‹è¯•å¤§å¼ é‡åˆ†é…
    try:
        # å°è¯•åˆ†é…å¤§å¼ é‡
        sizes_mb = [100, 500, 1000, 2000]
        for size_mb in sizes_mb:
            size_bytes = size_mb * 1024 * 1024
            size_elements = size_bytes // 4  # float32 = 4 bytes
            
            try:
                tensor = torch.randn(size_elements, device=device)
                allocated = torch.cuda.memory_allocated(device) / (1024**3)
                print(f"   æˆåŠŸåˆ†é… {size_mb} MB å¼ é‡ï¼Œæ€»æ˜¾å­˜ä½¿ç”¨: {allocated:.2f} GB")
                del tensor
                torch.cuda.empty_cache()
            except RuntimeError as e:
                print(f"   âŒ æ— æ³•åˆ†é… {size_mb} MB å¼ é‡: {e}")
                break
    except Exception as e:
        print(f"   æ˜¾å­˜æµ‹è¯•å¤±è´¥: {e}")

def test_ckip_gpu():
    """æµ‹è¯•CKIPåœ¨GPUä¸Šçš„è¿è¡Œ"""
    print("\nğŸ§  CKIP GPUæµ‹è¯•")
    print("=" * 50)
    
    try:
        from src.models.ckip_processor import CkipProcessor
        
        # æµ‹è¯•GPUåˆå§‹åŒ–
        print("æ­£åœ¨åˆå§‹åŒ–CKIPå¤„ç†å™¨ (GPUæ¨¡å¼)...")
        processor = CkipProcessor(model_name="bert-base", device="cuda:0")
        
        # æµ‹è¯•æ–‡æœ¬å¤„ç†
        test_text = "å¼—å†œÂ·å¾·æ€æœ­å…ˆç”Ÿåœ¨ä¸€å®¶åå«æ ¼æœ—å®çš„å…¬å¸åšä¸»ç®¡ï¼Œå…¬å¸ç”Ÿäº§é’»æœºã€‚"
        print(f"æµ‹è¯•æ–‡æœ¬: {test_text}")
        
        result = processor.process_text(test_text)
        
        print("âœ… CKIP GPUæµ‹è¯•æˆåŠŸ!")
        print(f"åˆ†è¯ç»“æœ: {len(result['tokens'])} ä¸ªè¯å…ƒ")
        print(f"å®ä½“è¯†åˆ«: {len(result['entities'])} ä¸ªå®ä½“")
        
        # æ˜¾ç¤ºGPUä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / (1024**3)
            reserved = torch.cuda.memory_reserved(0) / (1024**3)
            print(f"GPUæ˜¾å­˜ä½¿ç”¨: {allocated:.2f} GB (å·²åˆ†é…) / {reserved:.2f} GB (å·²ä¿ç•™)")
        
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥CKIPå¤„ç†å™¨ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä¾èµ–")
    except Exception as e:
        print(f"âŒ CKIP GPUæµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ™ºèƒ½æ’ç‰ˆç³»ç»Ÿ - GPUæ£€æµ‹å’Œæµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥PyTorchç‰ˆæœ¬
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}")
    
    # GPUæ£€æµ‹
    gpu_available = check_gpu()
    
    if gpu_available:
        # GPUæ€§èƒ½æµ‹è¯•
        test_gpu_performance()
        
        # CKIP GPUæµ‹è¯•
        test_ckip_gpu()
        
        print("\nâœ… GPUæ£€æµ‹å’Œæµ‹è¯•å®Œæˆ!")
        print("\nğŸ’¡ å»ºè®®:")
        print("1. ä½¿ç”¨ --device cuda:0 å‚æ•°å¯ç”¨GPUåŠ é€Ÿ")
        print("2. å¯¹äº4090Dï¼Œå»ºè®®ä½¿ç”¨è¾ƒå¤§çš„batch_size")
        print("3. ç›‘æ§æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼Œé¿å…OOMé”™è¯¯")
    else:
        print("\nâŒ GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        print("ğŸ’¡ å»ºè®®å®‰è£…CUDAå’ŒPyTorch GPUç‰ˆæœ¬ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½")

if __name__ == "__main__":
    main() 